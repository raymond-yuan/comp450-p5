{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymondyuan/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/raymondyuan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tqdm import tqdm, trange\n",
    "import collections\n",
    "\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.2121357 ,  0.03012651], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.envs.make(\"MountainCarContinuous-v0\")\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(2,), Box(1,), (-inf, inf))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space, env.reward_range,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to converte a state to a featurizes represenation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_state(state):\n",
    "    \"\"\"\n",
    "    Returns the featurized representation for a state.\n",
    "    \"\"\"\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_state(env.observation_space.sample()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCritic(models.Model):\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 num_eps=50):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.env = env\n",
    "        self.num_eps = num_eps\n",
    "        self.mu_layer = layers.Dense(1)\n",
    "        self.sigma_layer = layers.Dense(1)\n",
    "        self.value_layer = layers.Dense(1, name='value')\n",
    "        self.call(env.observation_space.sample())\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "\n",
    "        self.actor_ws = [self.mu_layer.weights, self.sigma_layer.weights]\n",
    "        self.critic_ws = self.value_layer.weights\n",
    "        self.all_ws = self.actor_ws + [self.critic_ws]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = featurize_state(inputs)\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        \n",
    "        self.value = self.value_layer(x)\n",
    "        self.value = tf.squeeze(self.value)\n",
    "        \n",
    "        self.mu = self.mu_layer(x)\n",
    "        self.sigma = tf.nn.softplus(self.sigma_layer(x))\n",
    "\n",
    "        self.mu = tf.squeeze(self.mu)\n",
    "        self.sigma = tf.squeeze(self.sigma)\n",
    "        self.normal_dist = tf.contrib.distributions.Normal(self.mu, self.sigma)\n",
    "        self.action = self.normal_dist._sample_n(1)\n",
    "        self.action = tf.clip_by_value(self.action, env.action_space.low[0], env.action_space.high[0])\n",
    "        return self.action, self.value\n",
    "\n",
    "    def get_grads(self, t, policy_target, value_target):\n",
    "        self.a_loss = -self.normal_dist.log_prob(self.action) * policy_target\n",
    "        # Add cross entropy cost to encourage exploration\n",
    "        self.a_loss -= 1e-1 * self.normal_dist.entropy()\n",
    "        \n",
    "        self.v_loss = tf.math.squared_difference(self.value, value_target)\n",
    "        \n",
    "        self.loss = self.a_loss + 0.01 * self.v_loss\n",
    "        return t.gradient(self.loss, self.weights)\n",
    "#         actor_grads = t.gradient(self.a_loss, self.actor_ws)\n",
    "#         value_grads = t.gradient(self.v_loss, self.critic_ws)\n",
    "#         return actor_grads, value_grads\n",
    "    \n",
    "    def train(self):\n",
    "        for ep in range(self.num_eps):\n",
    "            tr = tqdm(itertools.count())\n",
    "            state = self.env.reset()\n",
    "            total_r = 0\n",
    "            for t in tr:\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    a, v = self.call(state)\n",
    "\n",
    "                    next_state, r, done, info = self.env.step(a)\n",
    "                    total_r += r\n",
    "                    td_target = r + 0.99 * self.call(next_state)[1]\n",
    "                    td_error = td_target - v\n",
    "                \n",
    "                    grads = self.get_grads(tape, td_error, td_target)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.weights))\n",
    "                if done: break\n",
    "                \n",
    "                tr.set_description(f\"Ep {ep}/{self.num_eps} | Reward {total_r} | step {t}\")\n",
    "                state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -0.47605725464339943 | step 6: 7it [00:00, 62.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -1.286872712108213 | step 15: 16it [00:00, 67.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -2.0953437037638483 | step 24: 25it [00:00, 72.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -2.9473616754572625 | step 33: 34it [00:00, 76.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -3.8473616754572633 | step 42: 43it [00:00, 79.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -4.7473616754572605 | step 51: 52it [00:00, 82.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -5.647361675457257 | step 60: 61it [00:00, 82.66it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -6.4473616754572545 | step 68: 69it [00:00, 80.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -7.347361675457251 | step 77: 78it [00:00, 81.34it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -8.247361675457249 | step 86: 87it [00:01, 82.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -9.147361675457246 | step 95: 96it [00:01, 84.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -10.147361675457242 | step 105: 106it [00:01, 86.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -11.047361675457239 | step 114: 115it [00:01, 86.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -11.947361675457236 | step 123: 124it [00:01, 86.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -12.847361675457233 | step 132: 133it [00:01, 86.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -13.74736167545723 | step 141: 142it [00:01, 86.36it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -14.647361675457226 | step 150: 151it [00:01, 86.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -15.647361675457223 | step 160: 161it [00:01, 88.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -16.54736167545723 | step 169: 170it [00:01, 87.61it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -17.44736167545724 | step 178: 179it [00:02, 87.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -18.347361675457254 | step 187: 188it [00:02, 87.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -19.347361675457268 | step 197: 198it [00:02, 88.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -20.24736167545728 | step 206: 207it [00:02, 86.58it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -21.147361675457294 | step 215: 216it [00:02, 86.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -22.047361675457307 | step 224: 225it [00:02, 85.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -22.94736167545732 | step 233: 234it [00:02, 86.42it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -23.847361675457332 | step 242: 243it [00:02, 86.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -24.847361675457346 | step 252: 253it [00:02, 87.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -25.84736167545736 | step 262: 263it [00:03, 89.58it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -26.747361675457373 | step 271: 272it [00:03, 88.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -27.647361675457386 | step 280: 281it [00:03, 88.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -28.5473616754574 | step 289: 290it [00:03, 88.71it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -29.44736167545741 | step 298: 299it [00:03, 86.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -30.347361675457424 | step 307: 308it [00:03, 84.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -31.247361675457437 | step 316: 317it [00:03, 83.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -32.14736167545745 | step 325: 326it [00:03, 82.80it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -33.04736167545746 | step 334: 335it [00:03, 82.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -33.94736167545747 | step 343: 344it [00:04, 84.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -34.947361675457486 | step 353: 354it [00:04, 85.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -35.8473616754575 | step 362: 363it [00:04, 86.02it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -36.74736167545751 | step 371: 372it [00:04, 86.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -37.647361675457525 | step 380: 381it [00:04, 85.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -38.54736167545754 | step 389: 390it [00:04, 84.36it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -39.44736167545755 | step 398: 399it [00:04, 84.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Ep 0/50 | Reward -40.34736167545756 | step 407: 408it [00:04, 85.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4b866dde3a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-44246a4f5113>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mtotal_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0mtd_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.99\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd_target\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-44246a4f5113>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7a96e96b50b3>\u001b[0m in \u001b[0;36mfeaturize_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeaturized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeaturized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    820\u001b[0m         Xs = Parallel(n_jobs=self.n_jobs)(\n\u001b[1;32m    821\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m    823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;31m# All transformers are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;31m# if we have a weight for this transformer, multiply output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/kernel_approximation.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'random_weights_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_weights_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprojection\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_offset_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 568\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\r",
      "          \r",
      "Ep 0/50 | Reward -41.04736167545757 | step 414: 408it [00:15, 26.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "a = ActorCritic(env)\n",
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper([<tf.Tensor: id=1646, shape=(400, 1), dtype=float64, numpy=\n",
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])>, <tf.Tensor: id=1644, shape=(1,), dtype=float64, numpy=array([0.])>, <tf.Tensor: id=1663, shape=(400, 1), dtype=float64, numpy=\n",
       "array([[-7.57490311e-03],\n",
       "       [ 1.12403748e-02],\n",
       "       [-9.81424220e-03],\n",
       "       [ 1.12298156e-02],\n",
       "       [ 1.70266279e-03],\n",
       "       [-8.29114723e-03],\n",
       "       [-6.33488315e-03],\n",
       "       [-5.09622323e-03],\n",
       "       [ 9.82851424e-03],\n",
       "       [-5.66346691e-03],\n",
       "       [ 1.17085688e-03],\n",
       "       [-3.04202113e-03],\n",
       "       [ 6.70777376e-03],\n",
       "       [-9.74892482e-03],\n",
       "       [-1.89162829e-03],\n",
       "       [ 5.44492608e-03],\n",
       "       [-8.61136627e-03],\n",
       "       [ 1.06896867e-02],\n",
       "       [ 6.43051411e-03],\n",
       "       [ 1.04426399e-02],\n",
       "       [ 5.98360211e-03],\n",
       "       [-3.66370386e-03],\n",
       "       [-4.87052101e-03],\n",
       "       [ 7.84500309e-03],\n",
       "       [-1.90038144e-03],\n",
       "       [-3.95458417e-03],\n",
       "       [ 8.34870556e-03],\n",
       "       [ 7.55335801e-04],\n",
       "       [ 2.65150634e-03],\n",
       "       [-9.52066333e-03],\n",
       "       [ 3.66874146e-03],\n",
       "       [-9.84611690e-03],\n",
       "       [-4.15112401e-03],\n",
       "       [-8.26221875e-03],\n",
       "       [ 9.94795466e-03],\n",
       "       [-9.60685657e-03],\n",
       "       [-9.85147907e-03],\n",
       "       [-9.18283170e-03],\n",
       "       [-7.67085209e-05],\n",
       "       [-4.33042843e-03],\n",
       "       [ 5.77947837e-03],\n",
       "       [ 8.55226586e-03],\n",
       "       [ 5.76609569e-03],\n",
       "       [-3.57353972e-03],\n",
       "       [-9.72639618e-03],\n",
       "       [-6.91071912e-03],\n",
       "       [-1.08071687e-02],\n",
       "       [ 1.47676335e-03],\n",
       "       [-1.05494201e-02],\n",
       "       [-1.04895802e-02],\n",
       "       [-7.76972374e-03],\n",
       "       [ 1.12514105e-02],\n",
       "       [ 4.43308618e-03],\n",
       "       [-4.48642140e-04],\n",
       "       [-1.08014056e-02],\n",
       "       [ 1.12508238e-02],\n",
       "       [-3.70767708e-03],\n",
       "       [-9.88865453e-03],\n",
       "       [-2.45393790e-03],\n",
       "       [ 9.79164029e-03],\n",
       "       [-6.84594592e-03],\n",
       "       [-5.65103411e-04],\n",
       "       [ 1.11572320e-02],\n",
       "       [ 7.90470001e-03],\n",
       "       [-7.73619975e-03],\n",
       "       [ 1.01061166e-02],\n",
       "       [-1.35936540e-03],\n",
       "       [ 9.86974818e-03],\n",
       "       [-7.06914136e-03],\n",
       "       [ 4.86850557e-03],\n",
       "       [-1.02705632e-02],\n",
       "       [ 1.12474414e-02],\n",
       "       [-1.12499876e-02],\n",
       "       [-7.02272187e-04],\n",
       "       [-1.08592082e-02],\n",
       "       [-9.90749728e-03],\n",
       "       [ 1.11918244e-02],\n",
       "       [-3.85829355e-03],\n",
       "       [ 8.52813987e-03],\n",
       "       [ 1.09949584e-02],\n",
       "       [-2.43630976e-03],\n",
       "       [-1.06870403e-02],\n",
       "       [ 1.08719148e-02],\n",
       "       [ 1.04448391e-02],\n",
       "       [ 1.53106142e-03],\n",
       "       [-1.00348625e-02],\n",
       "       [ 9.59432682e-03],\n",
       "       [-8.40320907e-03],\n",
       "       [ 4.34026185e-03],\n",
       "       [-1.03025058e-02],\n",
       "       [ 5.17538768e-03],\n",
       "       [-1.12342462e-02],\n",
       "       [ 1.10527807e-02],\n",
       "       [-5.93904001e-03],\n",
       "       [ 5.54582245e-03],\n",
       "       [ 6.28354060e-03],\n",
       "       [ 7.61971146e-03],\n",
       "       [-8.46705675e-03],\n",
       "       [ 1.09843804e-02],\n",
       "       [-4.23372465e-03],\n",
       "       [ 9.31592740e-03],\n",
       "       [ 2.34931103e-03],\n",
       "       [ 1.04155813e-02],\n",
       "       [-1.11298786e-02],\n",
       "       [ 2.07849006e-03],\n",
       "       [ 1.09882480e-02],\n",
       "       [ 2.94151998e-03],\n",
       "       [ 7.54204055e-03],\n",
       "       [ 6.39756948e-03],\n",
       "       [-4.84770540e-03],\n",
       "       [ 1.11859293e-02],\n",
       "       [ 4.60375085e-03],\n",
       "       [ 6.84139540e-03],\n",
       "       [ 1.12110875e-02],\n",
       "       [-1.08730247e-02],\n",
       "       [-8.82524336e-03],\n",
       "       [-1.59903071e-03],\n",
       "       [ 1.07896628e-02],\n",
       "       [ 5.59052510e-03],\n",
       "       [-1.12188764e-02],\n",
       "       [-6.00733610e-03],\n",
       "       [ 3.02644631e-03],\n",
       "       [-5.59286003e-03],\n",
       "       [ 8.37743875e-03],\n",
       "       [ 7.49584838e-03],\n",
       "       [ 5.52899933e-04],\n",
       "       [ 1.11171184e-02],\n",
       "       [ 1.11547144e-02],\n",
       "       [ 4.76859824e-03],\n",
       "       [-9.24419142e-03],\n",
       "       [-1.02359081e-02],\n",
       "       [ 8.10998762e-03],\n",
       "       [ 2.62956674e-03],\n",
       "       [-1.06233493e-02],\n",
       "       [ 7.45381339e-03],\n",
       "       [ 1.07833445e-02],\n",
       "       [ 7.83984835e-03],\n",
       "       [ 7.74612846e-03],\n",
       "       [-1.10045358e-02],\n",
       "       [-1.10343789e-02],\n",
       "       [ 5.82361145e-04],\n",
       "       [-8.08087529e-03],\n",
       "       [ 3.15084928e-03],\n",
       "       [ 1.34171697e-03],\n",
       "       [ 1.10188363e-02],\n",
       "       [ 1.69883493e-03],\n",
       "       [-7.02581631e-03],\n",
       "       [-8.94647200e-03],\n",
       "       [ 1.12387744e-02],\n",
       "       [-7.81697369e-03],\n",
       "       [ 5.60511067e-04],\n",
       "       [-1.08042912e-02],\n",
       "       [ 1.04840833e-02],\n",
       "       [ 1.12340266e-02],\n",
       "       [-1.11203831e-02],\n",
       "       [ 9.12181261e-03],\n",
       "       [-1.01828735e-02],\n",
       "       [ 9.66393643e-03],\n",
       "       [ 8.37211459e-03],\n",
       "       [ 9.17823293e-03],\n",
       "       [-1.04483121e-02],\n",
       "       [-3.06353263e-03],\n",
       "       [-1.03239958e-02],\n",
       "       [ 1.43738545e-04],\n",
       "       [ 1.08169638e-02],\n",
       "       [ 2.42658379e-03],\n",
       "       [-1.12521464e-02],\n",
       "       [ 3.20565566e-03],\n",
       "       [ 1.03922352e-02],\n",
       "       [-2.46541090e-03],\n",
       "       [-8.50184946e-03],\n",
       "       [-1.12099267e-02],\n",
       "       [ 5.70917741e-03],\n",
       "       [-6.39573555e-03],\n",
       "       [-2.67361440e-03],\n",
       "       [-1.12524170e-02],\n",
       "       [ 3.01723527e-03],\n",
       "       [-1.06951100e-02],\n",
       "       [-6.30458190e-03],\n",
       "       [-1.12125574e-02],\n",
       "       [-1.03060207e-02],\n",
       "       [ 8.05823580e-03],\n",
       "       [ 1.55753545e-03],\n",
       "       [-4.04266871e-03],\n",
       "       [-8.54537138e-03],\n",
       "       [ 5.19916502e-03],\n",
       "       [ 1.26808144e-03],\n",
       "       [-9.92863206e-03],\n",
       "       [-3.27578128e-03],\n",
       "       [-3.53754664e-03],\n",
       "       [-8.96655716e-03],\n",
       "       [ 6.21418906e-03],\n",
       "       [ 9.09930747e-03],\n",
       "       [-8.73750133e-03],\n",
       "       [-8.46010112e-03],\n",
       "       [ 1.19809940e-03],\n",
       "       [-5.10301398e-03],\n",
       "       [-5.11021521e-03],\n",
       "       [ 7.25783035e-03],\n",
       "       [ 9.42805956e-03],\n",
       "       [-1.06572336e-02],\n",
       "       [-1.09684431e-02],\n",
       "       [-4.57501883e-03],\n",
       "       [ 4.40224809e-04],\n",
       "       [-9.62463826e-03],\n",
       "       [-1.12129627e-02],\n",
       "       [ 8.82490842e-03],\n",
       "       [-9.79830422e-03],\n",
       "       [ 5.13496094e-03],\n",
       "       [ 4.34314444e-03],\n",
       "       [-7.01275931e-03],\n",
       "       [ 6.25481660e-03],\n",
       "       [-1.06939228e-02],\n",
       "       [-9.93403799e-03],\n",
       "       [-2.29761975e-03],\n",
       "       [-8.25307230e-03],\n",
       "       [ 2.26565385e-03],\n",
       "       [ 3.72423487e-03],\n",
       "       [-7.75781165e-03],\n",
       "       [-7.70991827e-03],\n",
       "       [ 1.80703513e-03],\n",
       "       [-7.44265546e-03],\n",
       "       [ 4.08927995e-03],\n",
       "       [ 1.02255876e-02],\n",
       "       [ 1.12080187e-02],\n",
       "       [-8.54329158e-03],\n",
       "       [ 9.17778680e-03],\n",
       "       [-1.12389330e-02],\n",
       "       [-1.11033733e-02],\n",
       "       [ 5.69750000e-03],\n",
       "       [-4.53064658e-03],\n",
       "       [ 2.97922949e-03],\n",
       "       [-4.61730430e-03],\n",
       "       [-1.07195335e-02],\n",
       "       [-1.62848496e-03],\n",
       "       [-6.55082880e-04],\n",
       "       [ 1.01110326e-02],\n",
       "       [ 1.03059842e-02],\n",
       "       [-2.97128986e-04],\n",
       "       [-9.26740933e-03],\n",
       "       [ 6.03401203e-03],\n",
       "       [-1.02007985e-02],\n",
       "       [ 8.36818000e-03],\n",
       "       [ 8.04219299e-03],\n",
       "       [-7.97784502e-03],\n",
       "       [ 8.13481979e-03],\n",
       "       [ 7.70230585e-03],\n",
       "       [-1.07110392e-02],\n",
       "       [-6.34269891e-03],\n",
       "       [-8.88654851e-03],\n",
       "       [-8.79865137e-03],\n",
       "       [-7.61419818e-03],\n",
       "       [ 8.72934734e-03],\n",
       "       [-9.72487224e-03],\n",
       "       [ 5.47368563e-03],\n",
       "       [ 1.06108167e-02],\n",
       "       [ 8.82290029e-04],\n",
       "       [-1.10010306e-02],\n",
       "       [ 5.43038572e-03],\n",
       "       [-1.82399372e-03],\n",
       "       [-1.58175210e-03],\n",
       "       [ 6.79870264e-03],\n",
       "       [ 7.83259664e-03],\n",
       "       [-9.52878712e-03],\n",
       "       [ 9.59285031e-03],\n",
       "       [-1.69618970e-03],\n",
       "       [ 4.57118119e-03],\n",
       "       [-1.10988353e-02],\n",
       "       [-7.41948461e-04],\n",
       "       [-8.91299045e-03],\n",
       "       [-4.65079159e-03],\n",
       "       [-5.14211882e-03],\n",
       "       [ 8.61585629e-03],\n",
       "       [ 1.12518166e-02],\n",
       "       [ 1.26644839e-03],\n",
       "       [ 9.68541276e-03],\n",
       "       [ 6.75960314e-04],\n",
       "       [-6.43012952e-03],\n",
       "       [-6.60904009e-03],\n",
       "       [-7.24286137e-03],\n",
       "       [-2.69798803e-03],\n",
       "       [ 5.35437271e-04],\n",
       "       [ 7.12918546e-03],\n",
       "       [-8.59050494e-03],\n",
       "       [-1.12243838e-02],\n",
       "       [ 1.01213126e-02],\n",
       "       [ 5.94068175e-03],\n",
       "       [-9.30137382e-03],\n",
       "       [ 7.53899881e-03],\n",
       "       [-6.94192881e-03],\n",
       "       [-7.64565531e-03],\n",
       "       [-4.79285977e-03],\n",
       "       [-7.81090919e-03],\n",
       "       [ 1.06158452e-02],\n",
       "       [-1.10852962e-02],\n",
       "       [-7.81799561e-03],\n",
       "       [ 5.61427356e-03],\n",
       "       [-1.12501203e-02],\n",
       "       [ 1.07335100e-02],\n",
       "       [-2.16883617e-03],\n",
       "       [ 1.40655955e-03],\n",
       "       [ 5.19948508e-03],\n",
       "       [ 9.11770172e-03],\n",
       "       [ 1.46221736e-03],\n",
       "       [ 5.78789591e-03],\n",
       "       [ 5.33656707e-03],\n",
       "       [ 3.74433890e-03],\n",
       "       [-3.59003978e-03],\n",
       "       [ 3.80639330e-03],\n",
       "       [-3.83164675e-03],\n",
       "       [-5.27238937e-03],\n",
       "       [ 1.12455440e-02],\n",
       "       [-1.10207601e-02],\n",
       "       [ 1.11975441e-02],\n",
       "       [-8.81092910e-03],\n",
       "       [-2.19612411e-03],\n",
       "       [ 4.92938137e-03],\n",
       "       [-3.77643671e-05],\n",
       "       [-9.77237214e-03],\n",
       "       [ 1.12161264e-02],\n",
       "       [ 1.10475114e-02],\n",
       "       [-1.06884383e-02],\n",
       "       [-1.04203419e-02],\n",
       "       [ 9.36530336e-03],\n",
       "       [-2.44994582e-03],\n",
       "       [ 1.06755616e-02],\n",
       "       [-3.46138500e-03],\n",
       "       [ 1.11550655e-02],\n",
       "       [-7.54363112e-03],\n",
       "       [ 1.57995720e-03],\n",
       "       [ 1.04041378e-02],\n",
       "       [ 9.42796680e-03],\n",
       "       [-3.81575067e-03],\n",
       "       [ 1.49625158e-03],\n",
       "       [-4.58510796e-03],\n",
       "       [ 1.01961733e-02],\n",
       "       [-7.90103210e-03],\n",
       "       [ 9.94330737e-03],\n",
       "       [ 6.10986936e-03],\n",
       "       [-9.76086197e-03],\n",
       "       [-8.24151004e-03],\n",
       "       [-9.05350947e-03],\n",
       "       [-1.11035047e-02],\n",
       "       [-1.12509442e-02],\n",
       "       [ 1.10890960e-02],\n",
       "       [ 9.53964500e-03],\n",
       "       [ 1.01630642e-03],\n",
       "       [-1.11311097e-02],\n",
       "       [-8.96305000e-03],\n",
       "       [ 7.60348718e-03],\n",
       "       [ 9.60847914e-03],\n",
       "       [ 7.12629631e-03],\n",
       "       [ 1.31587523e-03],\n",
       "       [ 1.09215754e-02],\n",
       "       [-1.11832585e-02],\n",
       "       [-8.32107428e-03],\n",
       "       [-7.56039169e-03],\n",
       "       [-1.05800574e-02],\n",
       "       [ 2.43186072e-03],\n",
       "       [-1.01686505e-02],\n",
       "       [-8.61352042e-03],\n",
       "       [-1.09340572e-02],\n",
       "       [-7.33111299e-03],\n",
       "       [-6.51991929e-03],\n",
       "       [ 1.24340947e-03],\n",
       "       [ 5.94294305e-03],\n",
       "       [-6.40557822e-03],\n",
       "       [-5.71651771e-03],\n",
       "       [-4.68410630e-03],\n",
       "       [ 8.34041642e-03],\n",
       "       [ 8.97305480e-03],\n",
       "       [ 1.12453461e-02],\n",
       "       [-5.20505880e-03],\n",
       "       [-8.81658942e-03],\n",
       "       [ 9.06028148e-03],\n",
       "       [ 7.10350795e-03],\n",
       "       [-1.47090066e-03],\n",
       "       [-1.12451119e-02],\n",
       "       [ 1.11663465e-02],\n",
       "       [ 4.90311674e-03],\n",
       "       [ 4.74751655e-03],\n",
       "       [ 8.88270025e-03],\n",
       "       [-4.95827552e-04],\n",
       "       [-1.03714113e-02],\n",
       "       [-1.12392752e-02],\n",
       "       [ 1.00133559e-02],\n",
       "       [-5.61349125e-03],\n",
       "       [ 1.10593003e-02],\n",
       "       [-1.01506632e-02],\n",
       "       [-1.12427905e-02],\n",
       "       [ 8.77428683e-03],\n",
       "       [-1.10729678e-02],\n",
       "       [-1.08116538e-02],\n",
       "       [ 1.03487034e-02],\n",
       "       [-9.10866828e-03],\n",
       "       [ 1.12043724e-02],\n",
       "       [-3.99485507e-03],\n",
       "       [-8.97228717e-03],\n",
       "       [ 5.92943119e-03],\n",
       "       [ 8.79645668e-04]])>, <tf.Tensor: id=1661, shape=(1,), dtype=float64, numpy=array([-0.07956685])>, <tf.Tensor: id=1664, shape=(400, 1), dtype=float64, numpy=\n",
       "array([[-4.72478246e-03],\n",
       "       [ 7.56334809e-04],\n",
       "       [ 1.06535637e-03],\n",
       "       [ 5.32516655e-04],\n",
       "       [-1.57632580e-03],\n",
       "       [-1.42373882e-03],\n",
       "       [ 1.31444991e-03],\n",
       "       [ 9.40260610e-04],\n",
       "       [ 1.30130403e-04],\n",
       "       [-3.93410783e-04],\n",
       "       [-1.52921455e-03],\n",
       "       [-1.73005298e-03],\n",
       "       [-1.53523361e-03],\n",
       "       [-1.35181341e-03],\n",
       "       [ 8.56075980e-04],\n",
       "       [ 1.07022180e-03],\n",
       "       [-1.02455599e-03],\n",
       "       [ 1.19644329e-03],\n",
       "       [-4.04345942e-04],\n",
       "       [ 8.33194777e-05],\n",
       "       [ 6.48076443e-04],\n",
       "       [-3.09370590e-03],\n",
       "       [ 3.22843872e-03],\n",
       "       [ 8.01897858e-04],\n",
       "       [ 4.93039648e-03],\n",
       "       [ 3.23234698e-03],\n",
       "       [-5.60598864e-04],\n",
       "       [ 1.36437768e-03],\n",
       "       [-2.03615893e-03],\n",
       "       [-3.17000241e-03],\n",
       "       [-2.20243397e-03],\n",
       "       [-7.92000862e-04],\n",
       "       [ 1.04417876e-03],\n",
       "       [-2.04323398e-03],\n",
       "       [ 2.82552983e-03],\n",
       "       [ 1.77050412e-03],\n",
       "       [-1.71370300e-03],\n",
       "       [ 4.21537917e-04],\n",
       "       [ 1.38261544e-03],\n",
       "       [-3.12215536e-03],\n",
       "       [-1.09904893e-03],\n",
       "       [-2.55302337e-04],\n",
       "       [-2.79564714e-03],\n",
       "       [-1.25014356e-03],\n",
       "       [-2.96572513e-04],\n",
       "       [ 2.39350323e-03],\n",
       "       [-1.22496351e-03],\n",
       "       [ 2.90178026e-03],\n",
       "       [-7.51179101e-04],\n",
       "       [-1.24642972e-03],\n",
       "       [ 9.88695900e-04],\n",
       "       [ 6.18056529e-04],\n",
       "       [-1.80982261e-03],\n",
       "       [-2.62493635e-05],\n",
       "       [-9.77261860e-04],\n",
       "       [ 5.27621163e-04],\n",
       "       [ 1.38243150e-03],\n",
       "       [-1.48969551e-03],\n",
       "       [-1.89794137e-03],\n",
       "       [ 9.73913873e-04],\n",
       "       [ 1.55537510e-03],\n",
       "       [-2.56647590e-03],\n",
       "       [ 7.42942096e-04],\n",
       "       [-9.86647907e-04],\n",
       "       [ 3.91582803e-03],\n",
       "       [ 8.01304919e-04],\n",
       "       [-5.57752505e-03],\n",
       "       [ 5.97418548e-04],\n",
       "       [-1.11945736e-03],\n",
       "       [-2.10856988e-03],\n",
       "       [ 2.94436968e-04],\n",
       "       [ 6.47740100e-04],\n",
       "       [-6.83207204e-04],\n",
       "       [ 2.58305028e-03],\n",
       "       [-4.76335839e-04],\n",
       "       [-1.26554559e-04],\n",
       "       [ 2.11216023e-04],\n",
       "       [ 1.17936979e-03],\n",
       "       [ 1.08829892e-03],\n",
       "       [ 1.02993468e-03],\n",
       "       [ 3.91619403e-04],\n",
       "       [-5.97527579e-04],\n",
       "       [ 2.85640020e-04],\n",
       "       [ 1.34875977e-03],\n",
       "       [-2.50350665e-03],\n",
       "       [ 1.13930146e-03],\n",
       "       [ 1.66427839e-03],\n",
       "       [-2.33614345e-03],\n",
       "       [-3.19828898e-03],\n",
       "       [-1.20492101e-03],\n",
       "       [ 7.14236164e-05],\n",
       "       [-6.77548926e-04],\n",
       "       [ 8.25341412e-04],\n",
       "       [ 1.99221033e-03],\n",
       "       [ 4.31267120e-04],\n",
       "       [ 3.59846652e-03],\n",
       "       [-2.65697712e-03],\n",
       "       [-3.87879452e-04],\n",
       "       [ 2.47338455e-04],\n",
       "       [-1.40902111e-03],\n",
       "       [ 7.71486825e-04],\n",
       "       [-5.34509280e-04],\n",
       "       [ 4.59120573e-04],\n",
       "       [-4.39429548e-04],\n",
       "       [ 6.28032670e-04],\n",
       "       [ 6.86601826e-04],\n",
       "       [-3.98243223e-04],\n",
       "       [ 4.15084188e-04],\n",
       "       [-6.11490278e-04],\n",
       "       [-1.21166953e-05],\n",
       "       [ 8.16231352e-04],\n",
       "       [ 2.42576005e-03],\n",
       "       [-9.20176205e-04],\n",
       "       [ 7.91991379e-04],\n",
       "       [-8.15319181e-04],\n",
       "       [-1.85187715e-04],\n",
       "       [ 1.36532784e-03],\n",
       "       [ 8.40657303e-04],\n",
       "       [-5.53232434e-04],\n",
       "       [-4.53216585e-04],\n",
       "       [-1.28607039e-03],\n",
       "       [ 1.72622805e-03],\n",
       "       [-1.24293191e-03],\n",
       "       [-9.42448056e-04],\n",
       "       [ 8.17684708e-04],\n",
       "       [ 1.60402517e-04],\n",
       "       [ 4.17071125e-04],\n",
       "       [ 6.08072492e-04],\n",
       "       [-1.83499848e-03],\n",
       "       [-5.28808559e-04],\n",
       "       [-8.47919018e-04],\n",
       "       [ 5.98193039e-04],\n",
       "       [-1.73910012e-03],\n",
       "       [-1.43588445e-04],\n",
       "       [ 7.65465625e-04],\n",
       "       [ 6.07351517e-04],\n",
       "       [ 9.62898294e-04],\n",
       "       [ 2.52423883e-04],\n",
       "       [-1.05570773e-03],\n",
       "       [-1.21608338e-03],\n",
       "       [-5.03215791e-04],\n",
       "       [-7.85489873e-04],\n",
       "       [-9.62962158e-04],\n",
       "       [-2.02639538e-03],\n",
       "       [ 1.99635980e-04],\n",
       "       [ 3.23750721e-04],\n",
       "       [-1.54806185e-04],\n",
       "       [-1.01339083e-03],\n",
       "       [ 7.18839718e-04],\n",
       "       [-5.67174926e-04],\n",
       "       [ 1.39134024e-03],\n",
       "       [-4.60011634e-04],\n",
       "       [ 1.03192272e-03],\n",
       "       [ 7.87172313e-04],\n",
       "       [-8.69964739e-04],\n",
       "       [ 5.63447066e-04],\n",
       "       [-1.55615873e-03],\n",
       "       [ 1.62583703e-03],\n",
       "       [ 2.48647254e-04],\n",
       "       [ 1.82512469e-03],\n",
       "       [-4.26793067e-04],\n",
       "       [-7.68421757e-04],\n",
       "       [-7.87681229e-04],\n",
       "       [ 7.58529087e-04],\n",
       "       [ 1.01041826e-03],\n",
       "       [-7.10436311e-04],\n",
       "       [-6.45074089e-04],\n",
       "       [ 9.48402744e-04],\n",
       "       [ 5.89448700e-04],\n",
       "       [-6.74835406e-05],\n",
       "       [-8.98131696e-04],\n",
       "       [-7.26195683e-04],\n",
       "       [-4.27380544e-04],\n",
       "       [ 9.78311205e-04],\n",
       "       [ 1.74183038e-03],\n",
       "       [-6.75586627e-04],\n",
       "       [ 1.31828778e-03],\n",
       "       [-4.39151961e-04],\n",
       "       [-9.57116029e-04],\n",
       "       [-7.79168571e-04],\n",
       "       [-6.65678943e-04],\n",
       "       [ 6.49840611e-04],\n",
       "       [-1.70054265e-03],\n",
       "       [-2.21037459e-04],\n",
       "       [-3.67924815e-05],\n",
       "       [-5.26681016e-04],\n",
       "       [ 5.63171136e-04],\n",
       "       [ 1.56206992e-04],\n",
       "       [ 3.73418168e-04],\n",
       "       [ 1.30156844e-03],\n",
       "       [-2.81951331e-04],\n",
       "       [ 1.31191925e-03],\n",
       "       [-8.65721711e-05],\n",
       "       [-5.03962742e-04],\n",
       "       [-1.43482133e-03],\n",
       "       [ 8.42526088e-04],\n",
       "       [ 2.50294245e-04],\n",
       "       [-1.08708457e-03],\n",
       "       [ 2.72689459e-04],\n",
       "       [ 9.32125500e-04],\n",
       "       [-6.33939554e-04],\n",
       "       [-7.08883758e-04],\n",
       "       [ 2.90875014e-04],\n",
       "       [-1.39127216e-04],\n",
       "       [-9.03952283e-04],\n",
       "       [-4.50951852e-04],\n",
       "       [ 1.37850978e-03],\n",
       "       [-1.10033295e-03],\n",
       "       [-8.98653755e-04],\n",
       "       [ 1.23940139e-03],\n",
       "       [-7.53482725e-04],\n",
       "       [ 1.26957293e-03],\n",
       "       [-3.23582751e-04],\n",
       "       [-7.72146034e-04],\n",
       "       [ 5.17354851e-04],\n",
       "       [-1.64517090e-03],\n",
       "       [ 1.69258891e-03],\n",
       "       [ 1.56873721e-03],\n",
       "       [-6.54482208e-04],\n",
       "       [ 2.97399281e-04],\n",
       "       [ 7.31223019e-04],\n",
       "       [-9.36237634e-04],\n",
       "       [-2.60745346e-04],\n",
       "       [ 6.62302457e-04],\n",
       "       [ 5.69149974e-04],\n",
       "       [ 4.03443167e-04],\n",
       "       [ 1.75998183e-03],\n",
       "       [-6.63784003e-04],\n",
       "       [-7.07064712e-04],\n",
       "       [-5.40408489e-04],\n",
       "       [ 2.41297419e-04],\n",
       "       [ 2.76365452e-04],\n",
       "       [-9.03327956e-04],\n",
       "       [-4.02088336e-04],\n",
       "       [ 3.13431746e-05],\n",
       "       [-3.90465733e-04],\n",
       "       [ 4.22187533e-04],\n",
       "       [ 5.87797044e-04],\n",
       "       [ 5.24983305e-04],\n",
       "       [-9.06814988e-04],\n",
       "       [ 1.72264517e-03],\n",
       "       [-1.16711784e-03],\n",
       "       [ 3.29848739e-04],\n",
       "       [ 6.56744640e-04],\n",
       "       [-7.96443759e-04],\n",
       "       [ 7.34628781e-04],\n",
       "       [ 2.60514105e-05],\n",
       "       [-7.11996485e-04],\n",
       "       [-7.64894861e-04],\n",
       "       [-1.23696192e-03],\n",
       "       [-1.25718099e-03],\n",
       "       [ 1.39485932e-04],\n",
       "       [ 1.11030054e-03],\n",
       "       [ 3.86089664e-04],\n",
       "       [ 8.68534708e-04],\n",
       "       [ 9.85216552e-04],\n",
       "       [ 8.59193703e-05],\n",
       "       [-6.25499290e-04],\n",
       "       [-1.18790676e-03],\n",
       "       [ 1.61815935e-04],\n",
       "       [ 2.58280511e-04],\n",
       "       [ 5.87585243e-04],\n",
       "       [ 7.39676864e-04],\n",
       "       [-9.42888269e-04],\n",
       "       [ 1.50324208e-03],\n",
       "       [ 4.87960924e-05],\n",
       "       [ 4.43827883e-04],\n",
       "       [-6.33855026e-04],\n",
       "       [-6.16123205e-04],\n",
       "       [-5.79457005e-04],\n",
       "       [-4.50013538e-04],\n",
       "       [-3.36819952e-04],\n",
       "       [ 3.74925210e-04],\n",
       "       [ 6.78499290e-04],\n",
       "       [ 5.92073080e-04],\n",
       "       [ 3.85126116e-04],\n",
       "       [ 2.81618463e-04],\n",
       "       [-1.23699756e-04],\n",
       "       [-9.39161537e-05],\n",
       "       [-1.27452821e-03],\n",
       "       [-8.32673351e-04],\n",
       "       [-1.90686886e-03],\n",
       "       [ 9.15767144e-05],\n",
       "       [ 2.67730184e-04],\n",
       "       [-6.52125189e-04],\n",
       "       [-7.83158238e-05],\n",
       "       [-3.57749630e-04],\n",
       "       [-4.01857680e-04],\n",
       "       [ 1.02800451e-03],\n",
       "       [ 3.49769923e-04],\n",
       "       [ 3.02025883e-05],\n",
       "       [-1.85846767e-03],\n",
       "       [-4.26908265e-04],\n",
       "       [ 2.87869856e-04],\n",
       "       [-7.92745422e-04],\n",
       "       [ 9.13777509e-05],\n",
       "       [-6.86237571e-04],\n",
       "       [-6.96153046e-04],\n",
       "       [ 7.23210339e-04],\n",
       "       [ 2.13371676e-04],\n",
       "       [ 9.70531503e-05],\n",
       "       [-1.54636754e-04],\n",
       "       [ 2.28556124e-05],\n",
       "       [-4.17093539e-04],\n",
       "       [ 7.86202840e-04],\n",
       "       [ 6.24098058e-04],\n",
       "       [ 7.71478903e-04],\n",
       "       [ 3.86640010e-04],\n",
       "       [ 2.11327785e-04],\n",
       "       [ 7.13155577e-04],\n",
       "       [ 4.03813224e-04],\n",
       "       [ 6.65894155e-04],\n",
       "       [-7.45831677e-04],\n",
       "       [ 6.83211160e-04],\n",
       "       [-9.02312517e-04],\n",
       "       [-1.57845960e-03],\n",
       "       [ 4.03228561e-04],\n",
       "       [ 4.48337612e-04],\n",
       "       [-1.03731651e-04],\n",
       "       [ 6.30974007e-04],\n",
       "       [ 3.84388141e-04],\n",
       "       [-5.75021929e-04],\n",
       "       [-2.33201167e-04],\n",
       "       [ 8.29796966e-04],\n",
       "       [-1.75967362e-04],\n",
       "       [ 5.93045513e-04],\n",
       "       [-9.46142851e-05],\n",
       "       [ 7.63129471e-04],\n",
       "       [ 9.37480378e-05],\n",
       "       [ 5.16695722e-06],\n",
       "       [ 7.36161565e-04],\n",
       "       [ 2.33332606e-04],\n",
       "       [-2.52596476e-04],\n",
       "       [ 2.01563543e-04],\n",
       "       [-9.98914000e-04],\n",
       "       [ 6.19359283e-04],\n",
       "       [-8.75016801e-04],\n",
       "       [ 7.25071226e-04],\n",
       "       [-3.28285236e-04],\n",
       "       [-5.56580931e-04],\n",
       "       [-1.46983020e-04],\n",
       "       [-7.00704036e-05],\n",
       "       [-7.14957420e-04],\n",
       "       [-6.88744299e-04],\n",
       "       [ 8.30410656e-04],\n",
       "       [ 1.00770771e-03],\n",
       "       [-2.70455560e-04],\n",
       "       [-7.00216884e-04],\n",
       "       [-7.24193368e-04],\n",
       "       [ 9.49063433e-05],\n",
       "       [ 5.32401456e-04],\n",
       "       [-9.09138855e-05],\n",
       "       [-7.55805966e-04],\n",
       "       [ 7.90511117e-04],\n",
       "       [-7.30117542e-04],\n",
       "       [-9.45585701e-04],\n",
       "       [-8.71495592e-04],\n",
       "       [-6.74930685e-04],\n",
       "       [ 1.06997851e-04],\n",
       "       [-9.95283525e-04],\n",
       "       [-1.00478801e-04],\n",
       "       [-5.42661059e-04],\n",
       "       [-6.60452345e-04],\n",
       "       [-3.42924933e-04],\n",
       "       [ 2.78780389e-04],\n",
       "       [ 4.89069323e-04],\n",
       "       [ 5.22771800e-04],\n",
       "       [-3.89984715e-05],\n",
       "       [-4.05725456e-04],\n",
       "       [ 1.41947993e-04],\n",
       "       [ 6.72532494e-04],\n",
       "       [ 7.02754624e-04],\n",
       "       [-5.05505257e-04],\n",
       "       [-2.47974080e-05],\n",
       "       [ 1.17832778e-03],\n",
       "       [ 9.87353278e-05],\n",
       "       [ 1.26819630e-03],\n",
       "       [-6.91244418e-04],\n",
       "       [ 7.33813857e-04],\n",
       "       [ 3.12145015e-04],\n",
       "       [ 2.82007682e-04],\n",
       "       [ 9.17755784e-04],\n",
       "       [ 1.25521462e-03],\n",
       "       [-1.28699974e-03],\n",
       "       [-6.89962742e-04],\n",
       "       [ 3.61354829e-04],\n",
       "       [ 1.42961839e-04],\n",
       "       [ 6.53797108e-04],\n",
       "       [-6.03697792e-04],\n",
       "       [-6.67440882e-04],\n",
       "       [ 6.88781496e-04],\n",
       "       [-6.89984416e-04],\n",
       "       [-6.74649691e-04],\n",
       "       [ 5.13497076e-04],\n",
       "       [-4.90382613e-04],\n",
       "       [ 6.61431886e-04],\n",
       "       [ 1.33267335e-04],\n",
       "       [-3.31289868e-04],\n",
       "       [ 3.09582188e-04],\n",
       "       [ 6.64600126e-04]])>, <tf.Tensor: id=1665, shape=(1,), dtype=float64, numpy=array([-0.00483459])>])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self):\n",
    "        self.dense1 = layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = featurize_state(inputs)\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        x = self.dense1(x)\n",
    "        self.value = tf.squeeze(x)\n",
    "        return self.value\n",
    "    \n",
    "    def get_grads(self, t, target):\n",
    "        self.loss = tf.math.squared_difference(self.value, target)\n",
    "        return t.gradient(self.loss, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self):\n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just linear classifier\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy \n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized \n",
    "        estimator_value: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"  \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # env.render()\n",
    "            \n",
    "            # Take a step\n",
    "            action = estimator_policy.predict(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "#             stats.episode_rewards[i_episode] += reward\n",
    "#             stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = estimator_value.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - estimator_value.predict(state)\n",
    "            \n",
    "            # Update the value estimator\n",
    "            estimator_value.update(state, td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            estimator_policy.update(state, td_error, action)\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dennybritz/venv/py3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-16 13:31:05,772] From /Users/dennybritz/venv/py3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 662 @ Episode 50/50 (65.13252566564918))"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "policy_estimator = PolicyEstimator(learning_rate=0.001)\n",
    "value_estimator = ValueEstimator(learning_rate=0.1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Note, due to randomness in the policy the number of episodes you need varies\n",
    "    # TODO: Sometimes the algorithm gets stuck, I'm not sure what exactly is happening there.\n",
    "    stats = actor_critic(env, policy_estimator, value_estimator, 50, discount_factor=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
